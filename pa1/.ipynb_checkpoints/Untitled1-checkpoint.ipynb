{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b8e8cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ad' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 329\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRUNNING\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 329\u001b[0m     final_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal test accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 238\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    234\u001b[0m _ \u001b[38;5;241m=\u001b[39m temp_eval\u001b[38;5;241m.\u001b[39mrun(dummy_feed_dict)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# Now all nodes have their \"shape\" attributes set.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# --- Compute Gradients ---\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[43mad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_nodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# --- Create Evaluators for Training and Testing ---\u001b[39;00m\n\u001b[1;32m    241\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m ad\u001b[38;5;241m.\u001b[39mEvaluator([y_predict, loss] \u001b[38;5;241m+\u001b[39m grads)\n",
      "File \u001b[0;32m~/Desktop/cse234/cse234-w25-PA/pa1/auto_diff.py:958\u001b[0m, in \u001b[0;36mgradients\u001b[0;34m(output_node, nodes)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node\u001b[38;5;241m.\u001b[39mop, PlaceholderOp):\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 958\u001b[0m input_grad \u001b[38;5;241m=\u001b[39m \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, i_node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(node\u001b[38;5;241m.\u001b[39minputs):\n\u001b[1;32m    960\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i_node \u001b[38;5;129;01min\u001b[39;00m grads:\n",
      "File \u001b[0;32m~/Desktop/cse234/cse234-w25-PA/pa1/auto_diff.py:349\u001b[0m, in \u001b[0;36mSumOp.gradient\u001b[0;34m(self, node, output_grad)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m keepdim:\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;66;03m# Note: dims may be a tuple of dimensions along which the reduction happened.\u001b[39;00m\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;66;03m#WTF IS HAPPENINGs\u001b[39;00m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(dims):\n\u001b[0;32m--> 349\u001b[0m         grad \u001b[38;5;241m=\u001b[39m \u001b[43mad\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(grad, d)\n\u001b[1;32m    350\u001b[0m grad_shape \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39mattrs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(grad_shape, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ad' is not defined"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "from typing import Callable, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import auto_diff as ad\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "max_len = 28  # MNIST images are 28x28\n",
    "\n",
    "def transformer(X: ad.Node, nodes: List[ad.Node],\n",
    "                model_dim: int, seq_length: int, eps, batch_size, num_classes) -> ad.Node:\n",
    "    \"\"\"\n",
    "    Build a single transformer encoder layer for sequence classification.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ad.Node\n",
    "        Input node of shape (batch_size, seq_length, input_dim). (In our case, input_dim=28.)\n",
    "    nodes : List[ad.Node]\n",
    "        List of parameter nodes. We assume:\n",
    "            nodes[0]: W_Q (shape: [input_dim, model_dim])\n",
    "            nodes[1]: W_K (shape: [input_dim, model_dim])\n",
    "            nodes[2]: W_V (shape: [input_dim, model_dim])\n",
    "            nodes[3]: W_O (shape: [model_dim, model_dim])\n",
    "            nodes[4]: W_1 (shape: [model_dim, model_dim])\n",
    "            nodes[5]: W_2 (shape: [model_dim, num_classes])\n",
    "            nodes[6]: b_1 (shape: [model_dim,])\n",
    "            nodes[7]: b_2 (shape: [num_classes,])\n",
    "    model_dim : int\n",
    "        Hidden dimension.\n",
    "    seq_length : int\n",
    "        Sequence length (28 for MNIST).\n",
    "    eps : float\n",
    "        (Not used here, but passed for consistency.)\n",
    "    batch_size : int\n",
    "        Batch size.\n",
    "    num_classes : int\n",
    "        Number of output classes.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    logits : ad.Node\n",
    "        A node of shape (batch_size, num_classes) containing the classifier logits.\n",
    "    \"\"\"\n",
    "    # Unpack parameters\n",
    "    w_q = nodes[0]\n",
    "    w_k = nodes[1]\n",
    "    w_v = nodes[2]\n",
    "    w_o = nodes[3]\n",
    "    w_1 = nodes[4]\n",
    "    w_2 = nodes[5]\n",
    "    b_1 = nodes[6]\n",
    "    b_2 = nodes[7]\n",
    "\n",
    "    # --- Self-Attention ---\n",
    "    # Compute Q, K, V (each: [batch, seq_length, model_dim])\n",
    "    Q = ad.matmul(X, w_q)\n",
    "    K = ad.matmul(X, w_k)\n",
    "    V = ad.matmul(X, w_v)\n",
    "\n",
    "    # For computing dot–products we need to transpose K (swap dims 1 and 2)\n",
    "    K_t = ad.transpose(K, 1, 2)  # now K_t is [batch, model_dim, seq_length]\n",
    "\n",
    "    # Attention scores: [batch, seq_length, seq_length]\n",
    "    scores = ad.matmul(Q, K_t)\n",
    "\n",
    "    # Scale the scores by sqrt(model_dim)\n",
    "    scale = model_dim ** 0.5\n",
    "    scaled_scores = ad.div_by_const(scores, scale)\n",
    "\n",
    "    # Apply softmax on the last dimension to obtain attention weights\n",
    "    attn_weights = ad.softmax(scaled_scores, dim=-1)\n",
    "\n",
    "    # Compute attention output: weighted sum of V\n",
    "    context = ad.matmul(attn_weights, V)  # [batch, seq_length, model_dim]\n",
    "\n",
    "    # (Optional) Output projection: project context using W_O.\n",
    "    attn_output = ad.matmul(context, w_o)  # [batch, seq_length, model_dim]\n",
    "\n",
    "    # --- Pooling ---\n",
    "    # Average over the sequence dimension to get a (batch, model_dim) tensor.\n",
    "    pooled = ad.sum_op(attn_output, dim=1, keepdim=False)\n",
    "    avg_pooled = ad.div_by_const(pooled, seq_length)\n",
    "\n",
    "    # --- Feed-Forward Network ---\n",
    "    # First linear layer with bias and ReLU activation.\n",
    "    hidden_linear = ad.matmul(avg_pooled, w_1)\n",
    "    hidden_linear = ad.add(hidden_linear, b_1)\n",
    "    hidden = ad.relu(hidden_linear)\n",
    "\n",
    "    # Second linear layer (output logits)\n",
    "    logits_linear = ad.matmul(hidden, w_2)\n",
    "    logits = ad.add(logits_linear, b_2)\n",
    "\n",
    "    return logits\n",
    "\n",
    "def softmax_loss(Z: ad.Node, y_one_hot: ad.Node, batch_size: int) -> ad.Node:\n",
    "    \"\"\"\n",
    "    Compute average softmax loss given the logits and one-hot encoded labels.\n",
    "    \"\"\"\n",
    "    # Compute softmax probabilities\n",
    "    probs = ad.softmax(Z, dim=1)\n",
    "    # Compute natural log of probabilities\n",
    "    log_probs = ad.log(probs)\n",
    "    # Multiply element–wise with the one-hot targets\n",
    "    prod = ad.mul(y_one_hot, log_probs)\n",
    "    # Sum over classes for each example\n",
    "    loss_per_example = ad.sum_op(prod, dim=1, keepdim=False)\n",
    "    # Sum over the batch and take negative average\n",
    "    total_loss = ad.sum_op(loss_per_example, dim=0, keepdim=False)\n",
    "    avg_loss = ad.div_by_const(ad.mul_by_const(total_loss, -1), batch_size)\n",
    "    return avg_loss\n",
    "\n",
    "def sgd_epoch(\n",
    "    f_run_model: Callable,\n",
    "    X: torch.Tensor,\n",
    "    y: torch.Tensor,\n",
    "    model_weights: List[torch.Tensor],\n",
    "    batch_size: int,\n",
    "    lr: float,\n",
    ") -> Tuple[List[torch.Tensor], float]:\n",
    "    \"\"\"\n",
    "    Run one epoch of SGD.\n",
    "    \"\"\"\n",
    "    num_examples = X.shape[0]\n",
    "    num_batches = (num_examples + batch_size - 1) // batch_size\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min(start_idx + batch_size, num_examples)\n",
    "        if end_idx - start_idx == 0:\n",
    "            continue\n",
    "        X_batch = X[start_idx:end_idx]\n",
    "        y_batch = y[start_idx:end_idx]\n",
    "\n",
    "        # Run the forward and backward pass\n",
    "        outputs = f_run_model(model_weights, X_batch, y_batch)\n",
    "        # Expected outputs: [logits, loss, grad_w_q, grad_w_k, grad_w_v, grad_w_o, grad_w_1, grad_w_2, grad_b_1, grad_b_2]\n",
    "        loss_val = outputs[1]\n",
    "        grads = outputs[2:]\n",
    "\n",
    "        # Update each model parameter\n",
    "        for j in range(len(model_weights)):\n",
    "            model_weights[j] = model_weights[j] - lr * grads[j]\n",
    "\n",
    "        # Accumulate loss (weighted by the number of examples in the mini-batch)\n",
    "        total_loss += loss_val.item() * (end_idx - start_idx)\n",
    "\n",
    "    average_loss = total_loss / num_examples\n",
    "    print('Avg_loss:', average_loss)\n",
    "    return model_weights, average_loss\n",
    "\n",
    "def train_model():\n",
    "    \"\"\"\n",
    "    Train a single-layer transformer (ViT-style) on MNIST.\n",
    "    \"\"\"\n",
    "    # --- Hyperparameters ---\n",
    "    input_dim = 28      # Each row of an MNIST image has 28 pixels.\n",
    "    seq_length = max_len  # There are 28 rows per image.\n",
    "    num_classes = 10\n",
    "    model_dim = 128\n",
    "    eps = 1e-5\n",
    "\n",
    "    num_epochs = 20\n",
    "    batch_size = 50\n",
    "    lr = 0.02\n",
    "\n",
    "    # --- Build the Computational Graph ---\n",
    "    # Create input and ground-truth nodes.\n",
    "    X_node = ad.Variable(\"X\")\n",
    "    y_groundtruth = ad.Variable(\"y\")\n",
    "\n",
    "    # Create parameter nodes.\n",
    "    W_Q = ad.Variable(\"W_Q\")\n",
    "    W_K = ad.Variable(\"W_K\")\n",
    "    W_V = ad.Variable(\"W_V\")\n",
    "    W_O = ad.Variable(\"W_O\")\n",
    "    W_1 = ad.Variable(\"W_1\")\n",
    "    W_2 = ad.Variable(\"W_2\")\n",
    "    b_1 = ad.Variable(\"b_1\")\n",
    "    b_2 = ad.Variable(\"b_2\")\n",
    "    param_nodes = [W_Q, W_K, W_V, W_O, W_1, W_2, b_1, b_2]\n",
    "\n",
    "    # Build the transformer network (which outputs logits)\n",
    "    y_predict = transformer(X_node, param_nodes, model_dim, seq_length, eps, batch_size, num_classes)\n",
    "    # Define the softmax loss node.\n",
    "    loss = softmax_loss(y_predict, y_groundtruth, batch_size)\n",
    "\n",
    "    # --- Initialize Model Weights ---\n",
    "    np.random.seed(0)\n",
    "    stdv = 1.0 / np.sqrt(num_classes)\n",
    "    W_Q_val = np.random.uniform(-stdv, stdv, (input_dim, model_dim))\n",
    "    W_K_val = np.random.uniform(-stdv, stdv, (input_dim, model_dim))\n",
    "    W_V_val = np.random.uniform(-stdv, stdv, (input_dim, model_dim))\n",
    "    W_O_val = np.random.uniform(-stdv, stdv, (model_dim, model_dim))\n",
    "    W_1_val = np.random.uniform(-stdv, stdv, (model_dim, model_dim))\n",
    "    W_2_val = np.random.uniform(-stdv, stdv, (model_dim, num_classes))\n",
    "    b_1_val = np.random.uniform(-stdv, stdv, (model_dim,))\n",
    "    b_2_val = np.random.uniform(-stdv, stdv, (num_classes,))\n",
    "\n",
    "    model_weights = [\n",
    "        torch.tensor(W_Q_val, dtype=torch.float32),\n",
    "        torch.tensor(W_K_val, dtype=torch.float32),\n",
    "        torch.tensor(W_V_val, dtype=torch.float32),\n",
    "        torch.tensor(W_O_val, dtype=torch.float32),\n",
    "        torch.tensor(W_1_val, dtype=torch.float32),\n",
    "        torch.tensor(W_2_val, dtype=torch.float32),\n",
    "        torch.tensor(b_1_val, dtype=torch.float32),\n",
    "        torch.tensor(b_2_val, dtype=torch.float32)\n",
    "    ]\n",
    "\n",
    "    # --- Dummy Forward Pass to Set Node Shapes ---\n",
    "    dummy_feed_dict = {\n",
    "        X_node: torch.zeros(1, seq_length, input_dim),  # (1, 28, 28)\n",
    "        y_groundtruth: torch.zeros(1, num_classes),       # (1, 10)\n",
    "        W_Q: model_weights[0],\n",
    "        W_K: model_weights[1],\n",
    "        W_V: model_weights[2],\n",
    "        W_O: model_weights[3],\n",
    "        W_1: model_weights[4],\n",
    "        W_2: model_weights[5],\n",
    "        b_1: model_weights[6],\n",
    "        b_2: model_weights[7],\n",
    "    }\n",
    "    # Use a temporary evaluator to run the dummy forward pass.\n",
    "    temp_eval = ad.Evaluator([loss])\n",
    "    _ = temp_eval.run(dummy_feed_dict)\n",
    "    # Now all nodes have their \"shape\" attributes set.\n",
    "\n",
    "    # --- Compute Gradients ---\n",
    "    grads = ad.gradients(loss, param_nodes)\n",
    "\n",
    "    # --- Create Evaluators for Training and Testing ---\n",
    "    evaluator = ad.Evaluator([y_predict, loss] + grads)\n",
    "    test_evaluator = ad.Evaluator([y_predict])\n",
    "\n",
    "    # --- Load and Preprocess Data ---\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    train_dataset = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "    test_dataset = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "    # Reshape images to (num_examples, 28, 28) and normalize.\n",
    "    X_train = train_dataset.data.numpy().reshape(-1, 28, 28) / 255.0\n",
    "    y_train = train_dataset.targets.numpy()\n",
    "    X_test = test_dataset.data.numpy().reshape(-1, 28, 28) / 255.0\n",
    "    y_test = test_dataset.targets.numpy()\n",
    "\n",
    "    # One–hot encode the training labels.\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    y_train_oh = encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "\n",
    "    # --- Define Functions for Running the Graph ---\n",
    "    def f_run_model(model_weights, X_batch, y_batch):\n",
    "        feed_dict = {\n",
    "            X_node: X_batch,\n",
    "            y_groundtruth: y_batch,\n",
    "            W_Q: model_weights[0],\n",
    "            W_K: model_weights[1],\n",
    "            W_V: model_weights[2],\n",
    "            W_O: model_weights[3],\n",
    "            W_1: model_weights[4],\n",
    "            W_2: model_weights[5],\n",
    "            b_1: model_weights[6],\n",
    "            b_2: model_weights[7],\n",
    "        }\n",
    "        return evaluator.run(feed_dict)\n",
    "\n",
    "    def f_eval_model(X_val, model_weights: List[torch.Tensor]):\n",
    "        num_examples = X_val.shape[0]\n",
    "        num_batches = (num_examples + batch_size - 1) // batch_size\n",
    "        all_logits = []\n",
    "        for i in range(num_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min(start_idx + batch_size, num_examples)\n",
    "            if end_idx - start_idx == 0:\n",
    "                continue\n",
    "            X_batch = X_val[start_idx:end_idx]\n",
    "            feed_dict = {\n",
    "                X_node: X_batch,\n",
    "                W_Q: model_weights[0],\n",
    "                W_K: model_weights[1],\n",
    "                W_V: model_weights[2],\n",
    "                W_O: model_weights[3],\n",
    "                W_1: model_weights[4],\n",
    "                W_2: model_weights[5],\n",
    "                b_1: model_weights[6],\n",
    "                b_2: model_weights[7],\n",
    "            }\n",
    "            logits = test_evaluator.run(feed_dict)[0]\n",
    "            all_logits.append(logits)\n",
    "        concatenated_logits = np.concatenate(\n",
    "            [log.detach().numpy() if isinstance(log, torch.Tensor) else log for log in all_logits],\n",
    "            axis=0\n",
    "        )\n",
    "        predictions = np.argmax(concatenated_logits, axis=1)\n",
    "        return predictions\n",
    "\n",
    "    # --- Convert Data to Torch Tensors ---\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_train_oh = torch.tensor(y_train_oh, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.int64)\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    for epoch in range(num_epochs):\n",
    "        X_train, y_train_oh = shuffle(X_train, y_train_oh)\n",
    "        model_weights, loss_val = sgd_epoch(f_run_model, X_train, y_train_oh, model_weights, batch_size, lr)\n",
    "        pred_labels = f_eval_model(X_test, model_weights)\n",
    "        accuracy = np.mean(pred_labels == y_test)\n",
    "        print(f\"Epoch {epoch}: test accuracy = {accuracy}, loss = {loss_val}\")\n",
    "\n",
    "    # Final evaluation on test data.\n",
    "    final_predictions = f_eval_model(X_test, model_weights)\n",
    "    final_accuracy = np.mean(final_predictions == y_test)\n",
    "    return final_accuracy\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"RUNNING\")\n",
    "    final_acc = train_model()\n",
    "    print(f\"Final test accuracy: {final_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554fe473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9df237",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
